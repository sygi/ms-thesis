\chapter{Deep Q-learning}\label{dqn}
Deep Q-learning\footnote{known also as deep Q networks or DQN} is a reinforcement learning algorithm introduced in~\cite{nips-dqn}. It is known for being able to learn agents to play Atari games using the game screen as input. Mathematically, it is an extension of Q-learning (described in section~\ref{qlearning}), using neural networks as a Q-value approximator.

Even though Q-learning is a correct algorithm in a sense that it converges in the limit to the optimal strategy, the rate of this convergence is slow. It comes to no surprise---to propagate the rewards and correctly estimate Q-values of all states, each of them have to be visited multiple times.\footnote{\cite{qlearning-complexity} presents theoretical analysis of Q-learning speed of convergence. The raw Q-learning's complexity is estimated as $O(n^3)$ in the number of states.}
With raw Atari frames of size~$210 \times 160 \times 3$ (width $\times$ height $\times \{\text{R}, \text{G}, \text{B}\}$) and color depth of~$128$, the number of possible input states can be estimated as:
\begin{equation}
  \mbox{\# states} = 128^{210*160*3} \approx (2^7)^{2^{16}} \approx 2^{2^{18}} \approx 2^{256000}
\end{equation}\label{number-frame-states}
which is way to much to store in this universe, not to mention process multiple times on a computer.

Main idea of deep Q-learning is to use some form of generalization between states. This way, seeing a reward in a given state-action pair, we will get information not only about this particular state, but about similar states as well. This will also let us to store the Q-value function in a concise form, alleviating the exponential state explosion problem outlined above.

To model a problem with a neural network, we have to define a differential loss function to minimize (compare section~\ref{gradient descent}). In the case of deep Q-learning, a natural choice is some function of the difference between left- and right-hand sides of equation~\eqref{qlearning-property} (we know that the optimal strategies have this difference equal to~$0$), so a mean of squares of these differences was choosen:
\begin{equation}\label{dqn-loss}
  L(\theta_t) = \mathbb{E}_{s_t, a_t, s_{t+1}} \big(r_t + \gamma \max_b Q(s_{t+1}, b|\theta_{t-1}) - Q(s_t, a_t|\theta_t)\big)^2
\end{equation}
\todo{Put it as a definition and underline that $t$ comes from the context; say that we want to approximate it. Is there a theorem about the learning process with $L(\Theta_t)$? Maybe add that optimal strategies have this loss $0$.}
Let's explain notation in more detail. $Q$ is the function mapping state-action pairs to the estimation of their value (expected discounted reward). In our case the role of~$Q$ will be fulfilled by a neural network. The part~$|\theta_t$ comes from the fact that the values of~$Q$ function will be dependent on the current parameters $\theta_t$ of the network. $t$~is the discrete time in the episode. We define loss with regard to the parameters in time~$t$ ($L(\theta_t)$) using the current parameters~$\theta_{t-1}$---we want to be able to estimate ``goodness'' of any new parameters~($\theta_t$) based on~$\theta_{t-1}$. The estimation~sign~$\mathbb{E}_{s_t, a_t}$ comes from the fact we will not score all the state-action~pairs, but rather the ones that appear in real games. In fact, a more appropriate notation would be to sum over all the observed state-action~pairs; the current one is used to simplify notation.

We can differentiate the loss function with respect to the parameters:
\begin{multline}
  \nabla_{\theta_t} L(\theta_t) = \nabla_{\theta t}\, \mathbb{E}_{s_t, a_t, s_{t+1}} \big(r_t + \gamma \max_b Q(s_{t+1}, b|\theta_{t-1}) - Q(s_t, a_t|\theta_t)\big)^2
 =\\=
  \mathbb{E}_{s_t, a_t, s_{t+1}} \nabla_{\theta t} \big(r_t + \gamma \max_b Q(s_{t+1}, b|\theta_{t-1}) - Q(s_t, a_t|\theta_t)\big)^2
  =\\=
  \mathbb{E}_{a_t,s_t, s_{t+1}} 2\Big(
  r_t + \gamma \max_b Q(s_{t+1}, b|\theta_{t-1}) - Q(s_t, a_t|\theta_t)\Big)
  \nabla_{\theta_t} Q(s_t, a_t|\theta_t)
\end{multline}

Note that $\theta_{t-1}$ is treated as a constant with respect to~$\theta_t$. Even though this derivation is unnecessary to implement deep Q-learning (as it is usually done by a library), it gives some intuition about how our optimization algorithm will work.

$Q(s, a|\theta)$ will be implemented as a neural network that accepts the state of the game and returns a multidimensional vector---each coordinate will correspond to a Q-value of one action. At each time step, we will query the network with current state and make the action with the highest Q-value.
Then, we will collect tuples:~$s_t, a_t, r_t, s_{t+1}$ of current state, performed action, received (immediate) reward and the next state. To use such a tuple to decrease the loss~\eqref{dqn-loss}, we will calculate $\max_b Q(s_{t+1}, b)$ and~$Q(s_t, a)$ in a forward pass through the network, and $\nabla_{\theta_t} Q(s_t, a_t)$ using backpropagation (starting from the output node corresponding to the chosen action). We will use these expressions to calculate the derivative of the loss~$\nabla_{\theta_t} L(\theta_t)$ and update the parameters using RMSProp---a modification of gradient~descent described in section~\ref{rmsprop-section}.

The core algorithm, defined above, is not powerful enough to learn to successfully play Atari games using screen output. To make it work better, authors employed a couple of improvements, described below.

\section{Preprocessing the screen}
Low-detail graphics of Atari games have low information~density. If we entered the screens as they was to the network, it would have to learn the dependencies between pixels itself, leading to less stable and longer training. To circumvent this problem, authors of DQN downscale the screens to gray-scale~$110 \times 84$ images. Then, they further crop the central square of~$84 \times 84$ pixels of the image, to be able to easily pass it as input to a convolutional network.\footnote{Convolutional networks, as more appropriate to process screen than the main topic of this work---RAM, are not described in detail here. A detailed explanation of convnets can be found in chapter~9 of~\cite{dlbook}.}

\section{Exploration-exploitation trade-off}
When we play the game using Q-learning, we choose the actions that have the highest Q-values for the state we are in. For testing, that is a reasonable approach---we want our agent to play as good as he could, given current state value~estimation. But for training, the following scenario can happen: during our first game, say we chose action~$a_0$ in the initial state~$s_0$, proceeding to~$s_1$. We received some small, but non-zero reward~$r$ and continued the game from there. When we started the second game, we were obliged to use the action~$a_0$ in~$s_0$, as it is associated with non-zero reward (and we have little knowledge about the state value anyway). We will be doing this choice on and on, until the value estimation of state~$s_1$ drop, so that the small reward will be outweighed by it.

In this situation, our basically random choice at the start of training influences our behavior for a very long time, preventing us from trying other options, that may be more viable. This problem is known as exploration-exploitation trade-off; on one hand, we would like to make the best actions we know (to $\emph{exploit}$), but on the other, we would like to build our knowledge about the effects of different actions (to $\emph{explore}$).

One of the solution to this problem, is, instead of using a greedy strategy (choosing the actions with the best Q-values), to use so called $\varepsilon$-greedy~strategy---to choose a random action with probability~$\varepsilon$, and with probability~$1 - \varepsilon$ the one with the highest Q-value estimation. By choosing~$\varepsilon$ we decide how much time we want our agent to spend on exploring new actions vs. exploiting the ones he knows are good.

This technique is not uniquely associated with DQN; in fact it was used with bare Q-learning and other reinforcement learning algorithms from early on.

\discussed

\subsection{$\varepsilon$~decay}
To set the exploration-exploitation trade-off in an appropriate place, the DQN inventors decided to use changing~$\varepsilon$, namely one decaying linearly from~$1$ to~$0.1$. This way, at the beginning of training, when the state value estimations are poor, the agent does more exploration of new actions and further in the game, he chooses the best action according to the Q-values of states more often. Note that the random actions are done only in the training part---when the performance of a trained agent is evaluated, a flat, low value of $\varepsilon=0.05$ is used.

\section{Frameskip}\label{dqn-frameskip}
In many Atari games, developers use visual illusion to make games look better. For example, in Space Invaders, in which the aim of the user is to shoot down the enemy ships and avoid their shots, the shots are not shown on the screen on every frame. Instead, the user can see the shot on one frame, then for a couple of frames the shot cannot be seen on the screen, then it appears somewhere further. For humans, the illusion of movement is good enough.

If we tried to pass the one-frame input to the neural network, it'll have problems learning the appropriate Q-values, because without seeing the shot, the network will not know if in the next state the player will be hit and will lose a life or the enemy will be killed and user will gain points. Formally speaking, the markovian assumption we were using from the very beginning will be violated.

The solution to this problem is to pass not the last frame of the game, but the last $k$~frames (in~\cite{nips-dqn}, $k=4$ is used for most of the games) to the network, choose the action based on those frames and repeat it $k$~times.

This serves two purposes. First, it solves the problem of ``blinking'' objects, unless they disappear for at least $k$~frames.

Second, it helps for learning dynamics of the model. Let's imagine the agent is in a state, such that he will receive a big reward if he presses ``left'' for half a second. Assuming $A$~possible actions, $60$~frames per second, and basically random actions at the beginning, this will happen with probability~$A^{-(60 / 2)}$. If we change the action every $k=4$~frames, the probability to make the left movement for half a second will dramatically increase to~$A^{-(60/(4 \cdot 2))}$, as we will now make $4$~times less decisions.

Of course, this improvement (called frameskip) limits the amount of strategies that we can learn---our agents are now only able to express strategies that change actions at least every $k$~frames, but, unless the game need superhuman reflex (like, e.g.,~boxing~\cite{boxing}), using bigger than~$1$ frameskip value helps to train better agents in practice.

\section{Experience replay}\label{experience-replay}
Another method used to effectively train the neural network agents used in DQN is experience replay. Training a neural network to achieve good performance can be a long process. For a regular supervised tasks, like image recognition or machine translation is it not uncommon to have to process the whole dataset multiple (tens to hundreds) of times (called epochs) to get the optimal parameters for given hyperparameters, even with the use of minibatch training.

In the reinforcement learning setting, we don't have a dataset per se, we rather interact with environment and observe its reactions. To be able to reuse the training examples multiple times, we save the history of our interactions with environment (in a form of a list of tuples, containing $($state,~action,~reward,~next~state$)$) in the memory. During training, we do the actions according to the strategy defined by the network's Q-values estimation, but to update the weights, we choose a random batch of tuples from the memory and calculate the gradients using them.

This method allows us to look at each interaction (example) multiple times during training. It also improves statistical properties of the gradient: as we use stochastic gradient descent, we would like examples in each batch to be representative of the whole history of observations; choosing the elements of the batch randomly, we achieve better estimate of the full loss, as the consecutive observations are strongly correlated.

Due to the limits in the size of memory, the history of interactions contain last $n$ (in the order of tens-hundred to millions), not all past observations.
