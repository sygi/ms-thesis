\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
From the very beginning of computing, humanity is vividly interested in simulating the thought process of a mind through the field of artificial intelligence. The advances in theoretical aspects of computer science, as well as methods of manufacturing faster computers made the speed of progress of AI methods ever increasing. One can see it in many domains---from self-driving cars, through machine translation, to speech synthesis. A particularly neat area for testing intelligent algorithms is playing games.

Games are often used as a benchmark of the possibilities of AI, because on one hand, they have a well-defined evaluation metric and are cheap to simulate (which is not the case e.g. with medical diagnosis or steering robots) and, on the other hand, they offer limitless variants and levels of difficulty. While some of them (like chess, Go) require from playing agents quick search and accurate state evaluation \cite{alphago}, others (e.g.~Doom) need reflex and good object recognition, and some (e.g. Morpion) enjoy theorem-proving methods and being treated with linear programming \cite{morpion}.

In this work, we focus on games published for the Atari 2600---a game console from the previous century. While these are a very small subset of all games\footnote{There are around 470~games produced for Atari}, their variability offers a group of challenges. In opposition to the current trend \cite{nips-dqn, nature-dqn, a3c}, we train agents to play the games, seeing the RAM input instead of the screen. This problem is scarcely considered in literature (we are aware only of a poorly performing RAM agent in \cite{ale}); most of the methods described here are based on our previous work \cite{our-paper}, presented on Computer Games Workshop during International Joint Conference in Artificial Intelligence $2016$ in New~York.

In this thesis, we make a couple of contributions toward making better RAM-based Atari playing agents. First, we review the foundations of the models we will be using in chapter \ref{foundations}. They contain both ground work on reinforcement learning from 80s and~the evolving since $2012$ methods of building and training neural networks.

Second, in chapter \ref{dqn}, we define Deep Q-learning---the algorithm we use to train playing agents.
Chapter \ref{dqn-ram} consists of modification to original Deep Q-learning we tried to make it work for RAM input.
In chapter \ref{extensions} we show and discuss the results of evaluation of the trained methods.
Chapter \ref{conclusions} contains ideas for further extending this work.
In appendix \ref{icm} we describe the technical difficulties we overcame to feasibly train the models on the GPU cluster of our University's Interdisciplinary Centre for Mathematical and Computational Modelling.
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
First of all, I would like to thank my advisor, dr~hab. Henryk~Michalewski, for spending numerous hours to complete the project. He not only offered interesting scientific discussions, but also the help on the technical side.

I would also like to thank Marc G. Bellamare for suggesting the topic of the thesis and Deepsense.io for supporting our attendance at the IJCAI conference.

The computation performed within this project were carried out with the support of grant GG63-11 awarded by the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) University of Warsaw.
